from __future__ import annotations
import time
import warnings
from typing import Any

# Suppress deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Try to import from new package first, fallback to old one
try:
    from langchain_ollama import ChatOllama
    print("‚úÖ Using new langchain_ollama package")
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        print("‚ö†Ô∏è  Using deprecated langchain_community.chat_models")
        print("üí° Consider upgrading: pip install -U langchain-ollama")
    except ImportError:
        raise ImportError("‚ùå Neither langchain_ollama nor langchain_community.chat_models available")

from utils.ollama_manager import ensure_ollama_ready
from config import CONFIG


def get_llm(
    model: str | None = None,
    temperature: float = 0.3,
    timeout: int = 60,
    verbose: bool = False,
) -> ChatOllama:
    """
    Tr·∫£ v·ªÅ ChatOllama ƒë√£ s·∫µn s√†ng v·ªõi fallback models.
    """
    model = model or CONFIG["OLLAMA_MODELS"][0]
    
    # Th·ª≠ v·ªõi model ƒë∆∞·ª£c y√™u c·∫ßu
    try:
        ensure_ollama_ready(model)
        return ChatOllama(
            model=model,
            temperature=temperature,
            request_timeout=timeout,
            verbose=verbose,
        )
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ s·ª≠ d·ª•ng model {model}: {e}")
        
        # Fallback sang c√°c model kh√°c trong config
        fallback_models = [m for m in CONFIG["OLLAMA_MODELS"] if m != model]
        
        for fallback_model in fallback_models:
            try:
                print(f"üîÑ Th·ª≠ fallback model: {fallback_model}")
                ensure_ollama_ready(fallback_model)
                return ChatOllama(
                    model=fallback_model,
                    temperature=temperature,
                    request_timeout=timeout,
                    verbose=verbose,
                )
            except Exception as fallback_e:
                print(f"‚ùå Fallback model {fallback_model} c≈©ng th·∫•t b·∫°i: {fallback_e}")
                continue
        
        # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i, th·ª≠ v·ªõi c√°c model ph·ªï bi·∫øn
        common_models = ["llama3.2:1b", "llama3.2:3b", "qwen2:1.5b", "phi3:mini"]
        print("üîÑ Th·ª≠ v·ªõi c√°c model ph·ªï bi·∫øn...")
        
        for common_model in common_models:
            try:
                print(f"üîÑ Th·ª≠ model: {common_model}")
                ensure_ollama_ready(common_model)
                return ChatOllama(
                    model=common_model,
                    temperature=temperature,
                    request_timeout=timeout,
                    verbose=verbose,
                )
            except Exception:
                continue
        
        # N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c, raise l·ªói cu·ªëi c√πng
        raise RuntimeError(
            f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o b·∫•t k·ª≥ model n√†o. "
            f"H√£y ki·ªÉm tra:\n"
            f"  1. K·∫øt n·ªëi internet\n"
            f"  2. C√†i ƒë·∫∑t Ollama\n"
            f"  3. Th·ª≠ pull model thmanually: ollama pull llama3.2:1b"
        )


def call_llm(llm: ChatOllama, prompt: str, max_retry: int = 2) -> str:
    """
    G·ªçi LLM v·ªõi retry logic c·∫£i thi·ªán.
    """
    for attempt in range(1, max_retry + 2):       # 1 + retries
        try:
            response = llm.invoke(prompt)
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
        except Exception as exc:                  # pylint: disable=broad-except
            print(f"[LLM] Error (try {attempt}/{max_retry+1}): {exc}")
            if attempt > max_retry:
                return f"‚ö†Ô∏è Error generating response after {max_retry+1} attempts: {exc}"
            
            # TƒÉng th·ªùi gian ch·ªù theo s·ªë l·∫ßn th·ª≠
            sleep_time = 1.5 * attempt
            print(f"[LLM] Waiting {sleep_time}s before retry...")
            time.sleep(sleep_time)
    
    return "‚ö†Ô∏è Unexpected error in retry logic."